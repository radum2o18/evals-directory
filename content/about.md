---
title: About
description: A curated collection of LLM evaluation patterns, recipes, and examples across popular evaluation frameworks.
---

## Why This Exists

Evaluating LLM applications is hard. There are many frameworks, each with different approaches, APIs, and philosophies. Finding quality examples and patterns that work in production can be time-consuming.

This site aims to:

- **Collect proven patterns** - Real evaluation code that works in production
- **Compare frameworks** - Understand trade-offs between different tools
- **Accelerate development** - Copy, adapt, and ship faster
- **Share knowledge** - Learn from the community's experience

## What You'll Find

Each framework section contains:

- **Framework overview** - Deployment options, key features, and resources
- **Evaluation recipes** - Ready-to-use patterns for common use cases
- **Code examples** - Copy-paste code with explanations

## Covered Use Cases

- **RAG** - Retrieval quality, hallucination detection, answer completeness
- **Chatbot** - Response quality, safety, context adherence
- **Code Generation** - Syntax validation, functionality testing, security scans
- **Classification** - Accuracy metrics, multi-label evaluation
- **Prompt Engineering** - A/B testing, prompt comparison
- **Experimentation** - Experiment tracking, gradual rollouts

## Contributing

This is an open-source project. Contributions are welcome!

- Submit new evaluation patterns
- Improve existing examples
- Add support for new frameworks
- Fix bugs and typos

::card
---
title: Contribute on GitHub
icon: i-simple-icons-github
to: https://github.com/yourusername/evals-directory
target: _blank
---
::

## Built With

- [Nuxt](https://nuxt.com) - Vue framework
- [Nuxt UI](https://ui.nuxt.com) - UI components
- [Nuxt Content](https://content.nuxt.com) - Content management

