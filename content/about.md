---
title: About
description: A curated collection of LLM evaluation patterns across popular frameworks. Find production-ready examples to ship better AI faster.
navigation: false
---

# The Problem

You're building an AI application. You know you need evals. But where do you start?

Every framework has its own approach—different APIs, different philosophies, different trade-offs. The docs show basic examples, but finding patterns that actually work in production? That takes hours of digging through repos, blog posts, and Discord threads.

# The Solution

**Evals Directory** is a curated collection of evaluation patterns that you can copy, adapt, and ship.

No fluff. No theory. Just working code across the frameworks people actually use:

- **Braintrust** — Enterprise monitoring with experiment tracking
- **Evalite** — TypeScript-native evals built on Vitest  
- **LangSmith** — Deep LangChain integration with tracing
- **Promptfoo** — CLI-first testing with YAML configs

Each pattern includes the full code, explains what it tests, and tells you when to use it.

# Use Cases

Whether you're evaluating RAG pipelines, chatbot responses, code generation, or running prompt experiments—there's a pattern for that.

Browse by framework or search for what you need. Find something useful? Ship it.

# Open Source

This project is community-driven. If you've built an eval worth sharing, or found a bug worth fixing, contributions are welcome.

::card
---
title: Contribute on GitHub
icon: i-simple-icons-github
to: https://github.com/radum2o18/evals-directory
target: _blank
---
::

